{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nemotron-VLA: Minimal Training & Inference\n",
    "\n",
    "**Vision:** NVIDIA RADIO | **Language:** NVIDIA Nemotron Nano 9B v2 | **Action:** Diffusion Policy\n",
    "\n",
    "Dataset: [`keivalya/nemotron-vla-metaworld`](https://huggingface.co/datasets/keivalya/nemotron-vla-metaworld) | Model: [`keivalya/nemotron-vla`](https://huggingface.co/keivalya/nemotron-vla)\n",
    "\n",
    "> Helper functions : [`helpers.py`](./helpers.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "\n",
    "!pip install -q torch torchvision transformers accelerate \\\n",
    "    mujoco gymnasium metaworld \\\n",
    "    imageio[ffmpeg] imageio-ffmpeg \\\n",
    "    matplotlib datasets pyarrow Pillow \\\n",
    "    open_clip_torch timm tqdm\n",
    "\n",
    "# Nemotron Mamba dependencies\n",
    "!pip install -q causal-conv1d mamba-ssm\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from HuggingFace\n",
    "\n",
    "Downloads the expert demonstration dataset and extracts images, states, actions, and instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io, numpy as np, os\n",
    "\n",
    "print(\"Downloading dataset from HuggingFace...\")\n",
    "ds = load_dataset(\"keivalya/nemotron-vla-metaworld\", split=\"train\")\n",
    "print(f\"Total transitions: {len(ds)}\")\n",
    "\n",
    "# Extract arrays\n",
    "print(\"\\nExtracting data...\")\n",
    "images, states, actions, instructions = [], [], [], []\n",
    "for i, row in enumerate(ds):\n",
    "    img = np.array(Image.open(io.BytesIO(row[\"image\"])))\n",
    "    images.append(img)\n",
    "    states.append(np.array(row[\"state\"], dtype=np.float32))\n",
    "    actions.append(np.array(row[\"action\"], dtype=np.float32))\n",
    "    instructions.append(row[\"instruction\"])\n",
    "    if (i + 1) % 50000 == 0:\n",
    "        print(f\"  {i+1}/{len(ds)}\")\n",
    "\n",
    "images = np.stack(images)\n",
    "states = np.stack(states)\n",
    "actions = np.stack(actions)\n",
    "unique_instructions = list(set(instructions))\n",
    "\n",
    "print(f\"\\nLoaded: {images.shape[0]} transitions, {len(unique_instructions)} tasks\")\n",
    "print(f\"   Images:  {images.shape}\")\n",
    "print(f\"   States:  {states.shape}\")\n",
    "print(f\"   Actions: {actions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precompute Embeddings\n",
    "\n",
    "Sequentially load RADIO → extract vision features → unload, then load Nemotron → extract text embeddings → unload.\n",
    "This keeps GPU memory under 40GB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from helpers import load_radio, extract_radio_features, unload\n",
    "\n",
    "EMB_PATH = \"embeddings.npz\"\n",
    "if os.path.exists(EMB_PATH):\n",
    "    print(\"Embeddings already cached, loading...\")\n",
    "    emb = np.load(EMB_PATH, allow_pickle=True)\n",
    "    radio_features = emb[\"radio_features\"]\n",
    "    nemotron_embeddings = emb[\"nemotron_embeddings\"]\n",
    "    radio_dim = int(emb[\"radio_dim\"])\n",
    "    nemotron_dim = int(emb[\"nemotron_dim\"])\n",
    "else:\n",
    "    # RADIO vision features\n",
    "    print(\"\\nRADIO Vision Features\")\n",
    "    radio_model, radio_dim = load_radio(\"cuda\")\n",
    "    radio_features = extract_radio_features(radio_model, images, \"cuda\", batch_size=64)\n",
    "    unload(radio_model)\n",
    "\n",
    "    # Nemotron text embeddings\n",
    "    print(\"\\nNemotron Text Embeddings\")\n",
    "    from helpers import load_nemotron, extract_nemotron_embedding\n",
    "    nem_model, tok, nemotron_dim = load_nemotron(\"cuda\")\n",
    "\n",
    "    instr_to_emb = {}\n",
    "    for i, instr in enumerate(unique_instructions):\n",
    "        print(f\"  [{i+1}/{len(unique_instructions)}] \\\"{instr}\\\"\")\n",
    "        instr_to_emb[instr] = extract_nemotron_embedding(nem_model, tok, instr, \"cuda\")\n",
    "    unload(nem_model); del tok; torch.cuda.empty_cache()\n",
    "\n",
    "    nemotron_embeddings = np.stack([instr_to_emb[ins] for ins in instructions])\n",
    "\n",
    "    np.savez_compressed(EMB_PATH, radio_features=radio_features, nemotron_embeddings=nemotron_embeddings,\n",
    "                        radio_dim=radio_dim, nemotron_dim=nemotron_dim)\n",
    "    print(f\"\\nSaved embeddings: {EMB_PATH}\")\n",
    "\n",
    "print(f\"\\nRadio features:     {radio_features.shape} (dim={radio_dim})\")\n",
    "print(f\"Nemotron embeddings: {nemotron_embeddings.shape} (dim={nemotron_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Nemotron-VLA\n",
    "\n",
    "Only the lightweight fusion + diffusion head (~0.8M params) are trained. We keep the vision *(NVIDIA RADIO)* and language *(NVIDIA Nemotron Nano 9B v2)* encoders frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import NemotronVLA, VLADataset, train\n",
    "\n",
    "state_dim = states.shape[1]\n",
    "action_dim = actions.shape[1]\n",
    "\n",
    "dataset = VLADataset(radio_features, nemotron_embeddings, states, actions)\n",
    "print(f\"Dataset: {len(dataset)} samples\")\n",
    "\n",
    "model = NemotronVLA(\n",
    "    radio_dim=radio_dim, nemotron_dim=nemotron_dim,\n",
    "    state_dim=state_dim, action_dim=action_dim,\n",
    "    d_model=256, n_heads=4, diffusion_T=20,\n",
    ")\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {total:,}\")\n",
    "\n",
    "losses = train(model, dataset, epochs=80, batch_size=128, lr=3e-4,\n",
    "               device=\"cuda\", save_path=\"nemotron_vla.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, color=\"#76b900\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Nemotron-VLA Training\")\n",
    "plt.yscale(\"log\"); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Enter any MetaWorld task + instruction. The model generates actions and saves a video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, os\n",
    "from helpers import load_radio, load_nemotron, extract_nemotron_embedding, unload, NemotronVLA, run_inference\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Pre-encode instructions (Nemotron loaded once, then freed)\n",
    "TASKS = [\n",
    "    (\"push-v3\",         \"push the object to the goal\"),\n",
    "    (\"door-open-v3\",    \"open the door\"),\n",
    "    (\"drawer-close-v3\", \"close the drawer\"),\n",
    "    (\"window-open-v3\",  \"open the window\"),\n",
    "    (\"button-press-v3\", \"press the button\"),\n",
    "    (\"faucet-open-v3\",  \"open the faucet\"),\n",
    "    (\"reach-v3\",        \"reach to the target\"),\n",
    "    (\"pick-place-v3\",   \"pick and place the object\"),\n",
    "]\n",
    "\n",
    "print(\"Encoding instructions with Nemotron...\")\n",
    "nem_model, tok, _ = load_nemotron(device)\n",
    "cached = {}\n",
    "for env_name, instr in TASKS:\n",
    "    print(f\"  → \\\"{instr}\\\"\")\n",
    "    cached[instr] = extract_nemotron_embedding(nem_model, tok, instr, device)\n",
    "unload(nem_model); del nem_model, tok; torch.cuda.empty_cache()\n",
    "\n",
    "# ── Load VLA + RADIO ──\n",
    "ckpt = torch.load(\"nemotron_vla.pt\", map_location=device, weights_only=False)\n",
    "vla = NemotronVLA(**ckpt[\"config\"]).to(device)\n",
    "vla.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "vla.eval()\n",
    "print(f\"\\nVLA loaded (epoch {ckpt['epoch']}, loss {ckpt['loss']:.6f})\")\n",
    "\n",
    "radio, _ = load_radio(device)\n",
    "\n",
    "# ── Run all tasks ──\n",
    "os.makedirs(\"inference\", exist_ok=True)\n",
    "print(f\"\\n{'━'*50}\")\n",
    "for env_name, instr in TASKS:\n",
    "    run_inference(vla, radio, cached[instr], env_name, instr, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import HTML, display\n",
    "from base64 import b64encode\n",
    "\n",
    "for vf in sorted(glob.glob(\"inference/*.mp4\")):\n",
    "    with open(vf, \"rb\") as f:\n",
    "        b64 = b64encode(f.read()).decode()\n",
    "    name = os.path.basename(vf).replace(\".mp4\", \"\").replace(\"_\", \" \")\n",
    "    display(HTML(f'<h4>{name}</h4><video width=\"360\" controls loop><source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\"></video>'))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
