{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Nemotron-VLA: Vision-Language-Action Model with NVIDIA Foundation Models",
    "",
    "<div align=\"center\">",
    "",
    "**A fully-functional VLA powered by NVIDIA Nemotron & RADIO**",
    "",
    "*Vision: NVIDIA RADIO \u00b7 Language: NVIDIA Nemotron Nano 9B v2 \u00b7 Action: Diffusion Policy*",
    "",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](#)",
    "",
    "</div>",
    "",
    "---",
    "",
    "## Overview",
    "",
    "**Nemotron-VLA** is a Vision-Language-Action model that uses NVIDIA's foundation models as its backbone:",
    "",
    "| Modality | Model | Role | Parameters | Trainable? |",
    "|----------|-------|------|------------|------------|",
    "| \ud83d\uddbc\ufe0f Vision | **NVIDIA RADIO** | Image feature extractor | ~0.4-0.7B | \u2744\ufe0f Frozen |",
    "| \ud83d\udcdd Language | **NVIDIA Nemotron Nano 9B v2** | Text instruction encoder | ~9B | \u2744\ufe0f Frozen |",
    "| \ud83e\uddbe Action | **Diffusion Policy Head** | Action generation | ~0.5M | \ud83d\udd25 Trained |",
    "| \ud83d\udd00 Fusion | **Cross-Attention Module** | Multi-modal fusion | ~0.3M | \ud83d\udd25 Trained |",
    "",
    "### Architecture",
    "",
    "```",
    "  NVIDIA RADIO (ViT)    Nemotron Nano 9B v2    Robot State",
    "       \u2502                       \u2502                    \u2502",
    "   [frozen]                [frozen]             [trainable]",
    "       \u2502                       \u2502                    \u2502",
    "  Vision Proj \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Text Proj \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 State Encoder",
    "       \u2502                       \u2502                    \u2502",
    "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
    "                   \u2502",
    "          Cross-Attention Fusion",
    "                   \u2502",
    "         Diffusion Policy Head",
    "                   \u2502",
    "              Robot Actions",
    "```",
    "",
    "### Training Strategy",
    "",
    "To fit everything on an **A100 40GB** Colab GPU:",
    "1. **Collect** expert demonstrations from MetaWorld",
    "2. **Precompute** RADIO vision features (load \u2192 extract \u2192 unload)",
    "3. **Precompute** Nemotron text embeddings (load \u2192 extract \u2192 unload)",
    "4. **Train** only the lightweight fusion + diffusion modules",
    "",
    "> \ud83d\udca1 **Key insight:** By precomputing embeddings from the frozen NVIDIA models, we only need ~1M trainable parameters during the training loop, making it extremely memory efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 1. \ud83d\udd27 Setup & Installation",
    "",
    "First, let's install all required dependencies. This cell handles MetaWorld, MuJoCo, NVIDIA models, and rendering on Colab."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# 1.1 System dependencies for MuJoCo headless rendering\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "\n",
    "# Install BOTH EGL and OSMesa so auto-detection works everywhere\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq \\\n",
    "    libegl1-mesa-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglfw3 \\\n",
    "    libglew-dev \\\n",
    "    patchelf \\\n",
    "    > /dev/null 2>&1\n",
    "\n",
    "print(\"\u2705 System rendering libraries installed (EGL + OSMesa)\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# 1.2 Python packages\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "!pip install -q \\\n",
    "    torch torchvision \\\n",
    "    transformers accelerate \\\n",
    "    mujoco \\\n",
    "    gymnasium metaworld \\\n",
    "    imageio[ffmpeg] imageio-ffmpeg \\\n",
    "    matplotlib tqdm\n",
    "\n",
    "# For Nemotron's hybrid Mamba-2 architecture\n",
    "!pip install -q causal-conv1d mamba-ssm\n",
    "\n",
    "print(\"\\n\u2705 All Python packages installed\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# 1.3 Verify GPU & rendering backend\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.cuda.get_device_name(0)\n",
    "    mem = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"GPU:             {gpu}\")\n",
    "    print(f\"GPU Memory:      {mem:.1f} GB\")\n",
    "\n",
    "# \u2500\u2500 Rendering backend diagnostic \u2500\u2500\n",
    "import ctypes, os\n",
    "\n",
    "egl_ok, osmesa_ok = False, False\n",
    "try:\n",
    "    ctypes.cdll.LoadLibrary(\"libEGL.so\")\n",
    "    egl_ok = True\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    ctypes.cdll.LoadLibrary(\"libOSMesa.so\")\n",
    "    osmesa_ok = True\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nRendering backends:\")\n",
    "print(f\"  EGL:    {'\u2705 available' if egl_ok else '\u274c not found'}\")\n",
    "print(f\"  OSMesa: {'\u2705 available' if osmesa_ok else '\u274c not found'}\")\n",
    "\n",
    "# Auto-select: EGL preferred (hardware accel on NVIDIA GPUs)\n",
    "if egl_ok:\n",
    "    os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "    print(f\"\\n\u2192 Using EGL (hardware-accelerated, native on NVIDIA GPUs)\")\n",
    "elif osmesa_ok:\n",
    "    os.environ[\"MUJOCO_GL\"] = \"osmesa\"\n",
    "    os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
    "    print(f\"\\n\u2192 Using OSMesa (software rendering fallback)\")\n",
    "else:\n",
    "    raise RuntimeError(\"No rendering backend found! Install libegl1-mesa-dev or libosmesa6-dev\")\n",
    "\n",
    "# Verify MuJoCo imports cleanly with the chosen backend\n",
    "import mujoco\n",
    "print(f\"  MuJoCo {mujoco.__version__} loaded with MUJOCO_GL={os.environ['MUJOCO_GL']}\")\n",
    "print(\"\\n\ud83d\ude80 Ready to go!\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 2. \ud83d\udcc1 Write Helper Modules",
    "",
    "We organize the code into three helper files:",
    "- **`env.py`** \u2014 MetaWorld environment wrapper & data collection",
    "- **`models.py`** \u2014 RADIO encoder, Nemotron encoder, fusion, diffusion head, full VLA",
    "- **`utils.py`** \u2014 Dataset, training loop, evaluation, visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Create project directories\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "import os\n",
    "for d in [\"data\", \"checkpoints\", \"videos\", \"embeddings\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "print(\"\ud83d\udcc2 Project directories created\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `env.py` \u2014 Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile env.py\n",
    "\"\"\"\n",
    "Nemotron-VLA: Environment Wrapper\n",
    "=================================\n",
    "MetaWorld MT1 environment wrapper for robot manipulation tasks.\n",
    "Compatible with Meta-World's gymnasium interface.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def _setup_rendering():\n",
    "    \"\"\"\n",
    "    Auto-detect the best MuJoCo rendering backend.\n",
    "    - EGL:    hardware-accelerated, works on NVIDIA GPUs (Colab, clusters)\n",
    "    - OSMesa: software fallback, needs libOSMesa.so\n",
    "    \"\"\"\n",
    "    # If user already set it, respect their choice\n",
    "    if \"MUJOCO_GL\" in os.environ:\n",
    "        return\n",
    "\n",
    "    # Try EGL first (native on NVIDIA GPUs, no PyOpenGL needed)\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.cdll.LoadLibrary(\"libEGL.so\")\n",
    "        os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "        return\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Try OSMesa as fallback\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.cdll.LoadLibrary(\"libOSMesa.so\")\n",
    "        os.environ[\"MUJOCO_GL\"] = \"osmesa\"\n",
    "        os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
    "        return\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Last resort: let MuJoCo figure it out (may need a display)\n",
    "    os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "\n",
    "_setup_rendering()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MetaWorldMT1Wrapper:\n",
    "    \"\"\"\n",
    "    Wraps a MetaWorld MT1 environment into a simple interface:\n",
    "      - reset() -> (image, state, info)\n",
    "      - step(action) -> (image, state, reward, done, info)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name=\"push-v3\",\n",
    "        seed=42,\n",
    "        render_mode=\"rgb_array\",\n",
    "        camera_name=\"corner2\",\n",
    "    ):\n",
    "        import gymnasium as gym\n",
    "        import metaworld  # noqa: F401 \u2014 registers Meta-World envs\n",
    "\n",
    "        self.env = gym.make(\n",
    "            \"Meta-World/MT1\",\n",
    "            env_name=env_name,\n",
    "            seed=seed,\n",
    "            render_mode=render_mode,\n",
    "            camera_name=camera_name,\n",
    "        )\n",
    "        self.render_mode = render_mode\n",
    "        self.env_name = env_name\n",
    "\n",
    "        obs, _ = self.env.reset()\n",
    "        self.state_dim = self._extract_state(obs).shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.obs_shape = self._get_image().shape\n",
    "\n",
    "    def _extract_state(self, obs):\n",
    "        \"\"\"Extract flat state vector from observation.\"\"\"\n",
    "        if isinstance(obs, dict):\n",
    "            if \"observation\" in obs:\n",
    "                state = obs[\"observation\"]\n",
    "            elif \"robot_state\" in obs or \"object_state\" in obs:\n",
    "                parts = []\n",
    "                if \"robot_state\" in obs:\n",
    "                    parts.append(obs[\"robot_state\"])\n",
    "                if \"object_state\" in obs:\n",
    "                    parts.append(obs[\"object_state\"])\n",
    "                state = np.concatenate(parts, axis=-1)\n",
    "            else:\n",
    "                raise KeyError(f\"Unknown obs keys: {list(obs.keys())}\")\n",
    "        else:\n",
    "            state = obs\n",
    "        return np.asarray(state, dtype=np.float32)\n",
    "\n",
    "    def _get_image(self):\n",
    "        \"\"\"Render and return RGB image.\"\"\"\n",
    "        img = self.env.render()\n",
    "        return img.astype(np.uint8)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        obs, info = self.env.reset(seed=seed)\n",
    "        state = self._extract_state(obs)\n",
    "        image = self._get_image()\n",
    "        return image, state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, truncate, terminate, info = self.env.step(action)\n",
    "        done = truncate or terminate\n",
    "        state = self._extract_state(obs)\n",
    "        image = self._get_image()\n",
    "        return image, state, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "def collect_demonstrations(\n",
    "    env_name=\"push-v3\",\n",
    "    camera_name=\"corner2\",\n",
    "    seed=42,\n",
    "    num_episodes=100,\n",
    "    max_steps=150,\n",
    "    instruction=\"push the object to the goal\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect expert demonstrations using MetaWorld's built-in policies.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: images, states, actions, instruction\n",
    "    \"\"\"\n",
    "    from metaworld.policies import ENV_POLICY_MAP\n",
    "    import gymnasium as gym\n",
    "    import metaworld  # noqa: F401\n",
    "\n",
    "    env = gym.make(\n",
    "        \"Meta-World/MT1\",\n",
    "        env_name=env_name,\n",
    "        seed=seed,\n",
    "        render_mode=\"rgb_array\",\n",
    "        camera_name=camera_name,\n",
    "    )\n",
    "\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    policy = ENV_POLICY_MAP[env_name]()\n",
    "\n",
    "    all_images, all_states, all_actions = [], [], []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = policy.get_action(obs)\n",
    "            img = env.render()\n",
    "\n",
    "            all_images.append(img.astype(np.uint8).copy())\n",
    "            all_states.append(np.asarray(obs, dtype=np.float32).ravel().copy())\n",
    "            all_actions.append(np.asarray(action, dtype=np.float32).copy())\n",
    "\n",
    "            obs, reward, truncate, terminate, info = env.step(action)\n",
    "            done = bool(truncate or terminate) or (int(info.get(\"success\", 0)) == 1)\n",
    "            steps += 1\n",
    "\n",
    "        success = int(info.get(\"success\", 0))\n",
    "        print(f\"  Episode {ep+1}/{num_episodes}: {steps} steps, success={success}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    data = {\n",
    "        \"images\": np.stack(all_images, axis=0),      # (N, H, W, 3)\n",
    "        \"states\": np.stack(all_states, axis=0),       # (N, state_dim)\n",
    "        \"actions\": np.stack(all_actions, axis=0),     # (N, action_dim)\n",
    "        \"instruction\": instruction,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n\u2705 Collected {data['images'].shape[0]} transitions\")\n",
    "    print(f\"   images:  {data['images'].shape}\")\n",
    "    print(f\"   states:  {data['states'].shape}\")\n",
    "    print(f\"   actions: {data['actions'].shape}\")\n",
    "\n",
    "    return data\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 `models.py` \u2014 Model Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile models.py\n",
    "\"\"\"\n",
    "Nemotron-VLA: Model Components\n",
    "===============================\n",
    "Vision-Language-Action model powered by NVIDIA foundation models:\n",
    "  - Vision:   NVIDIA RADIO (frozen feature extractor)\n",
    "  - Language:  NVIDIA Nemotron Nano (frozen text encoder)\n",
    "  - Action:   Diffusion Policy head (trainable)\n",
    "  - Fusion:   Cross-attention module (trainable)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  1. VISION ENCODER \u2014 NVIDIA RADIO (frozen)                      \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def load_radio_model(device=\"cuda\", dtype=torch.float16):\n",
    "    \"\"\"\n",
    "    Load NVIDIA RADIO vision foundation model from HuggingFace.\n",
    "\n",
    "    RADIO (AM-RADIO) distills knowledge from multiple teacher models\n",
    "    (CLIP, DINOv2, SAM) into a single efficient ViT backbone.\n",
    "    It produces both a summary embedding and spatial features.\n",
    "\n",
    "    Returns:\n",
    "        model: RADIO model (frozen, eval mode)\n",
    "        radio_dim: int, dimension of the summary embedding\n",
    "    \"\"\"\n",
    "    from transformers import AutoModel\n",
    "\n",
    "    print(\"\ud83d\udcf8 Loading NVIDIA RADIO vision encoder...\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        \"nvidia/RADIO\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=dtype,\n",
    "    ).to(device).eval()\n",
    "\n",
    "    # Freeze all parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Detect output dimension with a dummy forward pass\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 3, 224, 224, device=device, dtype=dtype)\n",
    "        summary, spatial = model(dummy)\n",
    "        radio_dim = summary.shape[-1]\n",
    "\n",
    "    print(f\"   \u2705 RADIO loaded \u2014 summary dim: {radio_dim}\")\n",
    "    return model, radio_dim\n",
    "\n",
    "\n",
    "def extract_radio_features(radio_model, images_np, device=\"cuda\", batch_size=64):\n",
    "    \"\"\"\n",
    "    Extract RADIO summary embeddings for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        radio_model: Loaded RADIO model\n",
    "        images_np: numpy array (N, H, W, 3) uint8\n",
    "        device: torch device\n",
    "        batch_size: batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        features: numpy array (N, radio_dim) float32\n",
    "    \"\"\"\n",
    "    import torchvision.transforms as T\n",
    "\n",
    "    # RADIO preprocessing: resize to 224x224, ImageNet normalization\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    N = images_np.shape[0]\n",
    "    all_features = []\n",
    "\n",
    "    radio_model.eval()\n",
    "    dtype = next(radio_model.parameters()).dtype\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(start + batch_size, N)\n",
    "        batch_imgs = []\n",
    "        for i in range(start, end):\n",
    "            img_t = transform(images_np[i])\n",
    "            batch_imgs.append(img_t)\n",
    "\n",
    "        batch_tensor = torch.stack(batch_imgs).to(device=device, dtype=dtype)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary, _ = radio_model(batch_tensor)\n",
    "\n",
    "        all_features.append(summary.float().cpu())\n",
    "\n",
    "        if (start // batch_size) % 10 == 0:\n",
    "            print(f\"   RADIO features: {end}/{N}\")\n",
    "\n",
    "    features = torch.cat(all_features, dim=0).numpy()\n",
    "    print(f\"   \u2705 Extracted RADIO features: {features.shape}\")\n",
    "    return features\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  2. LANGUAGE ENCODER \u2014 NVIDIA Nemotron Nano 9B v2 (frozen)      \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def load_nemotron_model(device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load NVIDIA Nemotron Nano 9B v2 for text embedding extraction.\n",
    "\n",
    "    Uses the hybrid Mamba-2 + Transformer architecture.\n",
    "    Loaded in float16 for efficient inference on A100.\n",
    "\n",
    "    Returns:\n",
    "        model: Nemotron model\n",
    "        tokenizer: Nemotron tokenizer\n",
    "        hidden_dim: int, hidden dimension of the model\n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "    print(f\"\ud83e\udde0 Loading {model_name} for text encoding...\")\n",
    "    print(\"   (This may take a few minutes to download ~18GB)\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Detect hidden dimension from model config\n",
    "    hidden_dim = model.config.hidden_size\n",
    "    print(f\"   \u2705 Nemotron loaded \u2014 hidden dim: {hidden_dim}\")\n",
    "\n",
    "    return model, tokenizer, hidden_dim\n",
    "\n",
    "\n",
    "def extract_nemotron_embedding(model, tokenizer, text, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Extract a text embedding from Nemotron by mean-pooling\n",
    "    the last hidden layer's representations.\n",
    "\n",
    "    Args:\n",
    "        model: Nemotron model\n",
    "        tokenizer: Nemotron tokenizer\n",
    "        text: instruction string\n",
    "\n",
    "    Returns:\n",
    "        embedding: numpy array (hidden_dim,) float32\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs.get(\"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        # Take the last hidden state and mean-pool over tokens\n",
    "        last_hidden = outputs.hidden_states[-1]  # (1, seq_len, hidden_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            embedding = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            embedding = last_hidden.mean(dim=1)\n",
    "\n",
    "    embedding = embedding.squeeze(0).float().cpu().numpy()\n",
    "    print(f\"   \u2705 Nemotron text embedding: {embedding.shape}\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def unload_model(model):\n",
    "    \"\"\"Free GPU memory by deleting model and clearing cache.\"\"\"\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"   \ud83d\uddd1\ufe0f  Model unloaded, GPU memory freed\")\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  3. STATE ENCODER (trainable)                                    \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "class StateEncoder(nn.Module):\n",
    "    \"\"\"Encode robot proprioceptive state into a d_model-dim vector.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int, d_model: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, d_model),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"state: (B, state_dim) -> (B, d_model)\"\"\"\n",
    "        return self.ln(self.net(state))\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  4. CROSS-ATTENTION FUSION (trainable)                           \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuse vision, language, and state embeddings via cross-attention.\n",
    "\n",
    "    Upgrade from mini-VLA's simple MLP concatenation:\n",
    "    - Uses multi-head cross-attention to let modalities attend to each other\n",
    "    - Followed by a feedforward network for the final fused representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 256, n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stack vision, language, state as a 3-token sequence\n",
    "        # then use self-attention to fuse them\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feedforward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, vis_token, txt_token, state_token):\n",
    "        \"\"\"\n",
    "        Each input: (B, d_model)\n",
    "        Returns: (B, d_model) \u2014 fused context vector\n",
    "        \"\"\"\n",
    "        # Stack into sequence: (B, 3, d_model)\n",
    "        tokens = torch.stack([vis_token, txt_token, state_token], dim=1)\n",
    "\n",
    "        # Self-attention over the 3 modality tokens\n",
    "        attn_out, _ = self.self_attn(tokens, tokens, tokens)\n",
    "        tokens = self.norm1(tokens + attn_out)\n",
    "\n",
    "        # Feedforward\n",
    "        ff_out = self.ff(tokens)\n",
    "        tokens = self.norm2(tokens + ff_out)\n",
    "\n",
    "        # Mean-pool over the 3 tokens to get a single fused vector\n",
    "        fused = tokens.mean(dim=1)  # (B, d_model)\n",
    "        return fused\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  5. DIFFUSION POLICY HEAD (trainable)                            \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "@dataclass\n",
    "class DiffusionConfig:\n",
    "    T: int = 20                 # diffusion timesteps\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 0.02\n",
    "    action_dim: int = 4         # MetaWorld action dim\n",
    "    cond_dim: int = 256         # fused context dim\n",
    "\n",
    "\n",
    "def make_beta_schedule(cfg: DiffusionConfig):\n",
    "    \"\"\"Linear beta schedule for DDPM.\"\"\"\n",
    "    betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.T)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "    return betas, alphas, alpha_bar\n",
    "\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for diffusion timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        \"\"\"t: (B,) integer timesteps -> (B, dim)\"\"\"\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(\n",
    "            torch.linspace(math.log(1.0), math.log(1000.0), half, device=t.device)\n",
    "        )\n",
    "        args = t.float().unsqueeze(-1) * freqs.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[..., :1])], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ActionDenoiseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Noise prediction network: \u03b5_\u03b8(x_t, t, cond)\n",
    "    Predicts the noise added to the action at timestep t,\n",
    "    conditioned on the fused VLA context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DiffusionConfig, time_emb_dim: int = 64, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.time_emb = SinusoidalTimeEmbedding(time_emb_dim)\n",
    "\n",
    "        in_dim = cfg.action_dim + time_emb_dim + cfg.cond_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, cfg.action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t, cond):\n",
    "        \"\"\"\n",
    "        x_t:  (B, action_dim) noisy action\n",
    "        t:    (B,) integer timestep\n",
    "        cond: (B, cond_dim) fused context\n",
    "        \"\"\"\n",
    "        t_emb = self.time_emb(t)\n",
    "        x = torch.cat([x_t, t_emb, cond], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DiffusionPolicyHead(nn.Module):\n",
    "    \"\"\"\n",
    "    DDPM-based action generation head.\n",
    "    Learns to denoise random Gaussian noise into robot actions,\n",
    "    conditioned on the fused vision-language-state context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: DiffusionConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.denoise_net = ActionDenoiseNetwork(cfg)\n",
    "\n",
    "        betas, alphas, alpha_bar = make_beta_schedule(cfg)\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alpha_bar\", alpha_bar)\n",
    "\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        \"\"\"Forward diffusion: x_t = \u221a(\u1fb1_t) * x0 + \u221a(1-\u1fb1_t) * \u03b5\"\"\"\n",
    "        ab_t = self.alpha_bar[t].unsqueeze(-1)  # (B, 1)\n",
    "        return torch.sqrt(ab_t) * x0 + torch.sqrt(1.0 - ab_t) * noise\n",
    "\n",
    "    def loss(self, actions, cond):\n",
    "        \"\"\"\n",
    "        Training loss: MSE between predicted and true noise.\n",
    "\n",
    "        actions: (B, action_dim) ground-truth actions\n",
    "        cond:    (B, cond_dim)   fused context\n",
    "        \"\"\"\n",
    "        B = actions.size(0)\n",
    "        t = torch.randint(0, self.cfg.T, (B,), device=actions.device)\n",
    "        noise = torch.randn_like(actions)\n",
    "        x_t = self.q_sample(actions, t, noise)\n",
    "        eps_pred = self.denoise_net(x_t, t, cond)\n",
    "        return F.mse_loss(eps_pred, noise)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: generate actions from noise.\n",
    "\n",
    "        cond: (B, cond_dim)\n",
    "        Returns: (B, action_dim)\n",
    "        \"\"\"\n",
    "        B = cond.size(0)\n",
    "        x_t = torch.randn(B, self.cfg.action_dim, device=cond.device)\n",
    "\n",
    "        for step in reversed(range(self.cfg.T)):\n",
    "            t = torch.full((B,), step, device=cond.device, dtype=torch.long)\n",
    "            eps_pred = self.denoise_net(x_t, t, cond)\n",
    "\n",
    "            beta_t = self.betas[step]\n",
    "            alpha_t = self.alphas[step]\n",
    "            alpha_bar_t = self.alpha_bar[step]\n",
    "\n",
    "            # Predict x0, then compute x_{t-1}\n",
    "            x0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t)\n",
    "\n",
    "            if step > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "                x_t = torch.sqrt(alpha_t) * x0_pred + torch.sqrt(beta_t) * noise\n",
    "            else:\n",
    "                x_t = x0_pred\n",
    "\n",
    "        return x_t\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  6. NEMOTRON-VLA \u2014 Full VLA Model                               \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "class NemotronVLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Nemotron-VLA: Vision-Language-Action Model\n",
    "\n",
    "    Architecture:\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 NVIDIA RADIO \u2502   \u2502 NVIDIA Nemotron  \u2502   \u2502 Robot State  \u2502\n",
    "        \u2502 (frozen ViT) \u2502   \u2502 Nano 9B (frozen) \u2502   \u2502  (raw obs)   \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502                   \u2502                     \u2502\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 Vision Proj  \u2502   \u2502  Text Proj     \u2502   \u2502 State Encoder \u2502\n",
    "        \u2502 Linear\u2192LN   \u2502   \u2502  Linear\u2192LN     \u2502   \u2502  MLP\u2192LN       \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502                   \u2502                     \u2502\n",
    "               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502 Cross-Attn  \u2502\n",
    "                    \u2502   Fusion    \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502  Diffusion  \u2502\n",
    "                    \u2502 Policy Head \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                      \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n",
    "                      \u2502 Actions \u2502\n",
    "                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "    During training:  precomputed RADIO & Nemotron embeddings \u2192 trainable layers\n",
    "    During inference: RADIO runs online, Nemotron embedding is cached\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        radio_dim: int,\n",
    "        nemotron_dim: int,\n",
    "        state_dim: int,\n",
    "        action_dim: int = 4,\n",
    "        d_model: int = 256,\n",
    "        n_heads: int = 4,\n",
    "        diffusion_T: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Projection layers for frozen encoder outputs\n",
    "        self.vision_proj = nn.Sequential(\n",
    "            nn.Linear(radio_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "        )\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(nemotron_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "        )\n",
    "\n",
    "        # Trainable state encoder\n",
    "        self.state_encoder = StateEncoder(state_dim, d_model)\n",
    "\n",
    "        # Cross-attention fusion\n",
    "        self.fusion = CrossAttentionFusion(d_model, n_heads)\n",
    "\n",
    "        # Diffusion action head\n",
    "        diff_cfg = DiffusionConfig(\n",
    "            T=diffusion_T,\n",
    "            action_dim=action_dim,\n",
    "            cond_dim=d_model,\n",
    "        )\n",
    "        self.diffusion_head = DiffusionPolicyHead(diff_cfg)\n",
    "\n",
    "        # Save config for checkpoint\n",
    "        self.config = {\n",
    "            \"radio_dim\": radio_dim,\n",
    "            \"nemotron_dim\": nemotron_dim,\n",
    "            \"state_dim\": state_dim,\n",
    "            \"action_dim\": action_dim,\n",
    "            \"d_model\": d_model,\n",
    "            \"n_heads\": n_heads,\n",
    "            \"diffusion_T\": diffusion_T,\n",
    "        }\n",
    "\n",
    "    def _fuse(self, vis_emb, txt_emb, state):\n",
    "        \"\"\"Project and fuse all modalities.\"\"\"\n",
    "        vis = self.vision_proj(vis_emb)       # (B, d_model)\n",
    "        txt = self.text_proj(txt_emb)         # (B, d_model)\n",
    "        st = self.state_encoder(state)        # (B, d_model)\n",
    "        return self.fusion(vis, txt, st)      # (B, d_model)\n",
    "\n",
    "    def compute_loss(self, vis_emb, txt_emb, state, actions):\n",
    "        \"\"\"\n",
    "        Training forward pass.\n",
    "\n",
    "        Args:\n",
    "            vis_emb:  (B, radio_dim)    precomputed RADIO features\n",
    "            txt_emb:  (B, nemotron_dim) precomputed Nemotron features\n",
    "            state:    (B, state_dim)    robot proprioceptive state\n",
    "            actions:  (B, action_dim)   ground-truth expert actions\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar diffusion loss\n",
    "        \"\"\"\n",
    "        cond = self._fuse(vis_emb, txt_emb, state)\n",
    "        return self.diffusion_head.loss(actions, cond)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, vis_emb, txt_emb, state):\n",
    "        \"\"\"\n",
    "        Inference: generate action from current observation.\n",
    "\n",
    "        Args:\n",
    "            vis_emb: (1, radio_dim)    RADIO features for current image\n",
    "            txt_emb: (1, nemotron_dim) cached Nemotron text embedding\n",
    "            state:   (1, state_dim)    current robot state\n",
    "\n",
    "        Returns:\n",
    "            action: (1, action_dim)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        cond = self._fuse(vis_emb, txt_emb, state)\n",
    "        return self.diffusion_head.sample(cond)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 `utils.py` \u2014 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile utils.py\n",
    "\"\"\"\n",
    "Nemotron-VLA: Utilities\n",
    "=======================\n",
    "Dataset, training loop, evaluation, and video helpers.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  1. DATASET                                                      \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "class NemotronVLADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training Nemotron-VLA with precomputed embeddings.\n",
    "\n",
    "    Loads precomputed RADIO vision features and Nemotron text embedding,\n",
    "    along with robot states and expert actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, embeddings_path: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: path to .npz with images, states, actions, instruction\n",
    "            embeddings_path: path to .npz with radio_features, nemotron_embedding\n",
    "        \"\"\"\n",
    "        # Load raw demonstration data\n",
    "        raw = np.load(data_path, allow_pickle=True)\n",
    "        self.states = raw[\"states\"].astype(np.float32)    # (N, state_dim)\n",
    "        self.actions = raw[\"actions\"].astype(np.float32)   # (N, action_dim)\n",
    "\n",
    "        # Load precomputed embeddings\n",
    "        emb = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.vision_features = emb[\"radio_features\"].astype(np.float32)  # (N, radio_dim)\n",
    "        self.text_embedding = emb[\"nemotron_embedding\"].astype(np.float32)  # (nemotron_dim,)\n",
    "\n",
    "        assert len(self.states) == len(self.vision_features), \\\n",
    "            f\"Mismatch: {len(self.states)} states vs {len(self.vision_features)} vision features\"\n",
    "\n",
    "        print(f\"\ud83d\udce6 Dataset loaded: {len(self)} samples\")\n",
    "        print(f\"   vision features: {self.vision_features.shape}\")\n",
    "        print(f\"   text embedding:  {self.text_embedding.shape}\")\n",
    "        print(f\"   states:          {self.states.shape}\")\n",
    "        print(f\"   actions:         {self.actions.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vis = torch.from_numpy(self.vision_features[idx])\n",
    "        txt = torch.from_numpy(self.text_embedding.copy())  # same for all samples\n",
    "        state = torch.from_numpy(self.states[idx])\n",
    "        action = torch.from_numpy(self.actions[idx])\n",
    "        return vis, txt, state, action\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  2. TRAINING                                                     \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def train_nemotron_vla(\n",
    "    model,\n",
    "    dataset,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 128,\n",
    "    lr: float = 3e-4,\n",
    "    weight_decay: float = 1e-5,\n",
    "    device: str = \"cuda\",\n",
    "    save_path: str = \"checkpoints/nemotron_vla.pt\",\n",
    "    log_interval: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the Nemotron-VLA model.\n",
    "\n",
    "    Args:\n",
    "        model: NemotronVLA model\n",
    "        dataset: NemotronVLADataset\n",
    "        epochs: number of training epochs\n",
    "        batch_size: batch size\n",
    "        lr: learning rate\n",
    "        weight_decay: weight decay for AdamW\n",
    "        device: training device\n",
    "        save_path: path to save checkpoint\n",
    "        log_interval: print loss every N epochs\n",
    "\n",
    "    Returns:\n",
    "        losses: list of average loss per epoch\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # Cosine annealing scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=epochs,\n",
    "        eta_min=lr * 0.01,\n",
    "    )\n",
    "\n",
    "    losses = []\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    print(f\"\\n\ud83c\udfcb\ufe0f Training Nemotron-VLA for {epochs} epochs\")\n",
    "    print(f\"   batch_size={batch_size}, lr={lr}, samples={len(dataset)}\")\n",
    "    print(f\"   batches/epoch={len(loader)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for vis, txt, state, action in loader:\n",
    "            vis = vis.to(device)\n",
    "            txt = txt.to(device)\n",
    "            state = state.to(device)\n",
    "            action = action.to(device)\n",
    "\n",
    "            loss = model.compute_loss(vis, txt, state, action)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * vis.size(0)\n",
    "            n_samples += vis.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / n_samples\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"config\": model.config,\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": avg_loss,\n",
    "            }, save_path)\n",
    "\n",
    "        if (epoch + 1) % log_interval == 0 or epoch == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            print(f\"   Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"loss={avg_loss:.6f} | \"\n",
    "                  f\"best={best_loss:.6f} | \"\n",
    "                  f\"lr={current_lr:.2e}\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"\u2705 Training complete! Best loss: {best_loss:.6f}\")\n",
    "    print(f\"   Checkpoint saved to: {save_path}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  3. EVALUATION                                                   \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def evaluate_nemotron_vla(\n",
    "    model,\n",
    "    radio_model,\n",
    "    text_embedding,\n",
    "    env_name=\"push-v3\",\n",
    "    camera_name=\"corner2\",\n",
    "    seed=42,\n",
    "    num_episodes=5,\n",
    "    max_steps=150,\n",
    "    device=\"cuda\",\n",
    "    save_video=True,\n",
    "    video_dir=\"videos\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate Nemotron-VLA in the MetaWorld environment.\n",
    "\n",
    "    Args:\n",
    "        model: trained NemotronVLA\n",
    "        radio_model: RADIO vision model (frozen)\n",
    "        text_embedding: precomputed text embedding (nemotron_dim,)\n",
    "        env_name: MetaWorld task\n",
    "        num_episodes: number of eval episodes\n",
    "        max_steps: max steps per episode\n",
    "        device: torch device\n",
    "        save_video: whether to save episode videos\n",
    "        video_dir: directory for videos\n",
    "\n",
    "    Returns:\n",
    "        results: dict with rewards and success rates\n",
    "    \"\"\"\n",
    "    import torchvision.transforms as T\n",
    "\n",
    "    # Ensure rendering backend is configured before env import\n",
    "    from env import _setup_rendering\n",
    "    _setup_rendering()\n",
    "    from env import MetaWorldMT1Wrapper\n",
    "\n",
    "    # RADIO preprocessing\n",
    "    radio_transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(224, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    radio_dtype = next(radio_model.parameters()).dtype\n",
    "\n",
    "    # Prepare text embedding tensor (reused every step)\n",
    "    txt_emb = torch.from_numpy(text_embedding).float().unsqueeze(0).to(device)\n",
    "\n",
    "    # Create environment\n",
    "    env = MetaWorldMT1Wrapper(\n",
    "        env_name=env_name,\n",
    "        seed=seed,\n",
    "        render_mode=\"rgb_array\",\n",
    "        camera_name=camera_name,\n",
    "    )\n",
    "\n",
    "    if save_video:\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "        import imageio\n",
    "\n",
    "    model.eval()\n",
    "    radio_model.eval()\n",
    "\n",
    "    all_rewards = []\n",
    "    all_successes = []\n",
    "\n",
    "    print(f\"\\n\ud83c\udfaf Evaluating Nemotron-VLA on {env_name}\")\n",
    "    print(f\"   episodes={num_episodes}, max_steps={max_steps}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        img, state, info = env.reset()\n",
    "        ep_reward = 0.0\n",
    "        frames = [img.copy()]\n",
    "        success = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Extract RADIO features for current image\n",
    "            img_t = radio_transform(img).unsqueeze(0).to(device=device, dtype=radio_dtype)\n",
    "            with torch.no_grad():\n",
    "                summary, _ = radio_model(img_t)\n",
    "                vis_emb = summary.float()  # (1, radio_dim)\n",
    "\n",
    "            # Prepare state\n",
    "            state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "            # Generate action\n",
    "            with torch.no_grad():\n",
    "                action_t = model.act(vis_emb, txt_emb, state_t)\n",
    "\n",
    "            action_np = action_t.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Step environment\n",
    "            img, state, reward, done, info = env.step(action_np)\n",
    "            ep_reward += reward\n",
    "            frames.append(img.copy())\n",
    "\n",
    "            if int(info.get(\"success\", 0)) == 1:\n",
    "                success = True\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(ep_reward)\n",
    "        all_successes.append(success)\n",
    "\n",
    "        status = \"\u2705 SUCCESS\" if success else \"\u274c FAIL\"\n",
    "        print(f\"   Episode {ep+1}/{num_episodes}: \"\n",
    "              f\"reward={ep_reward:.2f}, steps={step+1}, {status}\")\n",
    "\n",
    "        # Save video\n",
    "        if save_video:\n",
    "            video_path = os.path.join(video_dir, f\"{env_name}_ep{ep+1:03d}.mp4\")\n",
    "            with imageio.get_writer(video_path, fps=20) as writer:\n",
    "                for f in frames:\n",
    "                    writer.append_data(f)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    results = {\n",
    "        \"mean_reward\": np.mean(all_rewards),\n",
    "        \"std_reward\": np.std(all_rewards),\n",
    "        \"success_rate\": np.mean(all_successes),\n",
    "        \"rewards\": all_rewards,\n",
    "        \"successes\": all_successes,\n",
    "    }\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"\ud83d\udcca Results:\")\n",
    "    print(f\"   Mean reward:  {results['mean_reward']:.2f} \u00b1 {results['std_reward']:.2f}\")\n",
    "    print(f\"   Success rate: {results['success_rate']*100:.1f}%\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  4. CHECKPOINT LOADING                                           \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def load_checkpoint(checkpoint_path, device=\"cuda\"):\n",
    "    \"\"\"Load a trained NemotronVLA from checkpoint.\"\"\"\n",
    "    from models import NemotronVLA\n",
    "\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg = ckpt[\"config\"]\n",
    "\n",
    "    model = NemotronVLA(\n",
    "        radio_dim=cfg[\"radio_dim\"],\n",
    "        nemotron_dim=cfg[\"nemotron_dim\"],\n",
    "        state_dim=cfg[\"state_dim\"],\n",
    "        action_dim=cfg[\"action_dim\"],\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        n_heads=cfg[\"n_heads\"],\n",
    "        diffusion_T=cfg[\"diffusion_T\"],\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\u2705 Loaded checkpoint from {checkpoint_path}\")\n",
    "    print(f\"   Epoch: {ckpt.get('epoch', '?')}, Loss: {ckpt.get('loss', '?'):.6f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "# \u2551  5. VISUALIZATION                                                \u2551\n",
    "# \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\n",
    "def plot_training_loss(losses, save_path=None):\n",
    "    \"\"\"Plot training loss curve.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(range(1, len(losses) + 1), losses, linewidth=2, color=\"#76b900\")  # NVIDIA green\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    ax.set_ylabel(\"Diffusion Loss\", fontsize=12)\n",
    "    ax.set_title(\"Nemotron-VLA Training Loss\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"   \ud83d\udcc8 Loss plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def display_sample_frames(data_path, n_frames=8):\n",
    "    \"\"\"Display sample frames from collected demonstrations.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    images = data[\"images\"]\n",
    "    N = len(images)\n",
    "\n",
    "    indices = np.linspace(0, N - 1, n_frames, dtype=int)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_frames, figsize=(2.5 * n_frames, 2.5))\n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(images[idx])\n",
    "        axes[i].set_title(f\"t={idx}\", fontsize=9)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Expert Demonstration Samples\", fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 3. \ud83c\udfae Collect Expert Demonstrations",
    "",
    "We use MetaWorld's built-in scripted expert policy for `push-v3` to collect demonstration trajectories. Each transition includes:",
    "- **Image**: RGB observation from the `corner2` camera",
    "- **State**: 39-dim robot proprioceptive state",
    "- **Action**: 4-dim action (x, y, z, gripper)",
    "- **Instruction**: \"push the object to the goal\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from env import collect_demonstrations\n",
    "import numpy as np\n",
    "\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Collect demonstrations\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "DATA_PATH = \"data/push_v3_demos.npz\"\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    print(f\"\u2705 Dataset already exists at {DATA_PATH}, skipping collection\")\n",
    "else:\n",
    "    print(\"\ud83c\udfae Collecting expert demonstrations...\")\n",
    "    data = collect_demonstrations(\n",
    "        env_name=\"push-v3\",\n",
    "        camera_name=\"corner2\",\n",
    "        seed=42,\n",
    "        num_episodes=100,        # 100 episodes of expert play\n",
    "        max_steps=150,           # up to 150 steps each\n",
    "        instruction=\"push the object to the goal\",\n",
    "    )\n",
    "\n",
    "    # Save dataset\n",
    "    np.savez_compressed(\n",
    "        DATA_PATH,\n",
    "        images=data[\"images\"],\n",
    "        states=data[\"states\"],\n",
    "        actions=data[\"actions\"],\n",
    "        instruction=data[\"instruction\"],\n",
    "    )\n",
    "    print(f\"\\n\ud83d\udcbe Dataset saved to {DATA_PATH}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Visualize some demonstration frames\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "from utils import display_sample_frames\n",
    "\n",
    "display_sample_frames(DATA_PATH, n_frames=8)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 4. \ud83e\udde0 Precompute NVIDIA Model Embeddings",
    "",
    "This is the key memory management strategy:",
    "1. **Load NVIDIA RADIO** \u2192 extract vision features for ALL images \u2192 unload",
    "2. **Load NVIDIA Nemotron 9B** \u2192 extract text embedding \u2192 unload",
    "3. **Save** all precomputed embeddings to disk",
    "",
    "This way, during training we only need the small trainable components in GPU memory!",
    "",
    "> \u23f1\ufe0f This step takes ~5-10 minutes (downloading models + forward passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extract RADIO Vision Features",
    "",
    "[NVIDIA RADIO (AM-RADIO)](https://huggingface.co/nvidia/RADIO) is a multi-teacher distilled vision model that combines knowledge from CLIP, DINOv2, and SAM into a single efficient backbone. We use it as a frozen feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import load_radio_model, extract_radio_features, unload_model\n",
    "\n",
    "EMBEDDINGS_PATH = \"embeddings/precomputed.npz\"\n",
    "DATA_PATH = \"data/push_v3_demos.npz\"\n",
    "\n",
    "if os.path.exists(EMBEDDINGS_PATH):\n",
    "    print(f\"\u2705 Embeddings already exist at {EMBEDDINGS_PATH}, skipping\")\n",
    "    emb = np.load(EMBEDDINGS_PATH, allow_pickle=True)\n",
    "    radio_dim = emb[\"radio_features\"].shape[1]\n",
    "    nemotron_dim = emb[\"nemotron_embedding\"].shape[0]\n",
    "    print(f\"   RADIO dim:    {radio_dim}\")\n",
    "    print(f\"   Nemotron dim: {nemotron_dim}\")\n",
    "else:\n",
    "    # Load demonstration images\n",
    "    data = np.load(DATA_PATH, allow_pickle=True)\n",
    "    images = data[\"images\"]  # (N, H, W, 3)\n",
    "    instruction = str(data[\"instruction\"])\n",
    "    print(f\"Loaded {len(images)} images and instruction: '{instruction}'\")\n",
    "\n",
    "    # \u2500\u2500 Step 1: RADIO Vision Features \u2500\u2500\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: NVIDIA RADIO Vision Features\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    radio_model, radio_dim = load_radio_model(device=\"cuda\", dtype=torch.float16)\n",
    "    radio_features = extract_radio_features(\n",
    "        radio_model, images, device=\"cuda\", batch_size=64\n",
    "    )\n",
    "    unload_model(radio_model)\n",
    "\n",
    "    print(f\"\\n\ud83d\udcf8 RADIO features shape: {radio_features.shape}\")\n",
    "    print(f\"   GPU memory after unload: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract Nemotron Text Embedding",
    "",
    "[NVIDIA Nemotron Nano 9B v2](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2) uses a hybrid **Mamba-2 + Transformer** architecture for high-throughput language understanding. We extract a text embedding by mean-pooling the last hidden layer.",
    "",
    "Since we use a single instruction for all episodes, we only need to compute this embedding **once**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not os.path.exists(EMBEDDINGS_PATH):\n",
    "    # \u2500\u2500 Step 2: Nemotron Text Embedding \u2500\u2500\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: NVIDIA Nemotron Nano 9B Text Embedding\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    from models import load_nemotron_model, extract_nemotron_embedding\n",
    "\n",
    "    nemotron_model, tokenizer, nemotron_dim = load_nemotron_model(device=\"cuda\")\n",
    "    nemotron_embedding = extract_nemotron_embedding(\n",
    "        nemotron_model, tokenizer, instruction, device=\"cuda\"\n",
    "    )\n",
    "    unload_model(nemotron_model)\n",
    "    del tokenizer\n",
    "\n",
    "    print(f\"\\n\ud83e\udde0 Nemotron embedding shape: {nemotron_embedding.shape}\")\n",
    "    print(f\"   GPU memory after unload: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "    # \u2500\u2500 Save all embeddings \u2500\u2500\n",
    "    np.savez_compressed(\n",
    "        EMBEDDINGS_PATH,\n",
    "        radio_features=radio_features,\n",
    "        nemotron_embedding=nemotron_embedding,\n",
    "        radio_dim=radio_dim,\n",
    "        nemotron_dim=nemotron_dim,\n",
    "    )\n",
    "    print(f\"\\n\ud83d\udcbe All embeddings saved to {EMBEDDINGS_PATH}\")\n",
    "    print(f\"   RADIO features:    {radio_features.shape}\")\n",
    "    print(f\"   Nemotron embedding: {nemotron_embedding.shape}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 5. \ud83c\udfcb\ufe0f Train Nemotron-VLA",
    "",
    "Now we train only the lightweight components:",
    "- **Vision projection**: Linear layer mapping RADIO features \u2192 d_model",
    "- **Text projection**: Linear layer mapping Nemotron embedding \u2192 d_model",
    "- **State encoder**: 2-layer MLP for robot state",
    "- **Cross-attention fusion**: Multi-head attention over modality tokens",
    "- **Diffusion policy head**: DDPM-based action generation",
    "",
    "Total trainable parameters: **~0.8M** \u2014 trains fast even without the A100 being fully utilized!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from models import NemotronVLA, count_parameters\n",
    "from utils import NemotronVLADataset, train_nemotron_vla\n",
    "\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Load dataset with precomputed embeddings\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "dataset = NemotronVLADataset(\n",
    "    data_path=\"data/push_v3_demos.npz\",\n",
    "    embeddings_path=\"embeddings/precomputed.npz\",\n",
    ")\n",
    "\n",
    "# Load embedding dimensions\n",
    "emb = np.load(\"embeddings/precomputed.npz\", allow_pickle=True)\n",
    "radio_dim = int(emb[\"radio_dim\"])\n",
    "nemotron_dim = int(emb[\"nemotron_dim\"])\n",
    "\n",
    "# Infer state and action dims from data\n",
    "raw = np.load(\"data/push_v3_demos.npz\", allow_pickle=True)\n",
    "state_dim = raw[\"states\"].shape[1]\n",
    "action_dim = raw[\"actions\"].shape[1]\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Model Configuration:\")\n",
    "print(f\"   RADIO dim:    {radio_dim}\")\n",
    "print(f\"   Nemotron dim: {nemotron_dim}\")\n",
    "print(f\"   State dim:    {state_dim}\")\n",
    "print(f\"   Action dim:   {action_dim}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Create Nemotron-VLA model\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "model = NemotronVLA(\n",
    "    radio_dim=radio_dim,\n",
    "    nemotron_dim=nemotron_dim,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    d_model=256,             # internal representation dim\n",
    "    n_heads=4,               # cross-attention heads\n",
    "    diffusion_T=20,          # diffusion denoising steps\n",
    ")\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\n\ud83d\udcca Nemotron-VLA Model:\")\n",
    "print(f\"   Total parameters:     {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Memory footprint:     ~{trainable_params * 4 / 1e6:.1f} MB (float32)\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Train!\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "losses = train_nemotron_vla(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    epochs=80,               # training epochs\n",
    "    batch_size=128,          # batch size (can increase on A100)\n",
    "    lr=3e-4,                 # learning rate\n",
    "    weight_decay=1e-5,       # AdamW weight decay\n",
    "    device=\"cuda\",\n",
    "    save_path=\"checkpoints/nemotron_vla.pt\",\n",
    "    log_interval=5,          # print every 5 epochs\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Plot training loss\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "from utils import plot_training_loss\n",
    "\n",
    "plot_training_loss(losses, save_path=\"checkpoints/loss_curve.png\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 6. \ud83c\udfaf Evaluate Nemotron-VLA",
    "",
    "Now we test the trained model in the MetaWorld `push-v3` environment. During inference:",
    "- **RADIO** runs online to process each new camera frame",
    "- **Nemotron** embedding is cached (loaded from disk)",
    "- The **fusion + diffusion head** generates actions in real-time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import load_radio_model\n",
    "from utils import load_checkpoint, evaluate_nemotron_vla\n",
    "\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Load trained model and RADIO for online inference\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load trained Nemotron-VLA\n",
    "vla_model = load_checkpoint(\"checkpoints/nemotron_vla.pt\", device=device)\n",
    "\n",
    "# Load RADIO for online vision processing\n",
    "radio_model, _ = load_radio_model(device=device, dtype=torch.float16)\n",
    "\n",
    "# Load cached text embedding\n",
    "emb = np.load(\"embeddings/precomputed.npz\", allow_pickle=True)\n",
    "text_embedding = emb[\"nemotron_embedding\"]\n",
    "\n",
    "print(f\"\\n\u2705 All models loaded for inference\")\n",
    "print(f\"   GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Run evaluation\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "results = evaluate_nemotron_vla(\n",
    "    model=vla_model,\n",
    "    radio_model=radio_model,\n",
    "    text_embedding=text_embedding,\n",
    "    env_name=\"push-v3\",\n",
    "    camera_name=\"corner2\",\n",
    "    seed=42,\n",
    "    num_episodes=10,\n",
    "    max_steps=150,\n",
    "    device=device,\n",
    "    save_video=True,\n",
    "    video_dir=\"videos\",\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "# Display a recorded video\n",
    "# \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import glob\n",
    "\n",
    "video_files = sorted(glob.glob(\"videos/*.mp4\"))\n",
    "if video_files:\n",
    "    # Display the first video\n",
    "    video_path = video_files[0]\n",
    "    with open(video_path, \"rb\") as f:\n",
    "        video_data = b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "    display(HTML(f'''\n",
    "    <h3>\ud83c\udfac Nemotron-VLA: {os.path.basename(video_path)}</h3>\n",
    "    <video width=\"480\" controls autoplay loop>\n",
    "        <source src=\"data:video/mp4;base64,{video_data}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    '''))\n",
    "else:\n",
    "    print(\"No videos found in videos/ directory\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "## 7. \ud83d\udcca Summary & Next Steps",
    "",
    "### What We Built",
    "",
    "**Nemotron-VLA** \u2014 a Vision-Language-Action model for robot manipulation using NVIDIA's foundation models:",
    "",
    "| Component | Model | Source |",
    "|-----------|-------|--------|",
    "| Vision Encoder | NVIDIA RADIO | [HuggingFace](https://huggingface.co/nvidia/RADIO) |",
    "| Language Encoder | NVIDIA Nemotron Nano 9B v2 | [HuggingFace](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2) |",
    "| Fusion Module | Cross-Attention (custom) | Trained from scratch |",
    "| Action Head | DDPM Diffusion Policy (custom) | Trained from scratch |",
    "",
    "### Key Design Decisions",
    "",
    "1. **Precomputed embeddings**: We extract features from RADIO and Nemotron offline, enabling training on A100 40GB",
    "2. **Cross-attention fusion**: Upgraded from mini-VLA's MLP concatenation to multi-head self-attention over modality tokens",
    "3. **Deeper diffusion head**: 3-layer noise prediction network with LayerNorm for better action quality",
    "4. **Memory efficiency**: Only ~0.8M trainable parameters, entire training fits in <4GB GPU memory",
    "",
    "### Possible Extensions",
    "",
    "- \ud83d\udd04 **Multi-task**: Train on multiple MetaWorld tasks (MT10/MT50)",
    "- \ud83d\udde3\ufe0f **Arbitrary instructions**: Load Nemotron at inference for free-form text",
    "- \ud83d\udcc8 **Action chunking**: Predict sequences of future actions",
    "- \ud83d\udd2c **LoRA fine-tuning**: Fine-tune RADIO or Nemotron with LoRA for domain adaptation",
    "- \ud83e\udd16 **Real robot**: Deploy on physical hardware with camera input",
    "",
    "---",
    "*Built with \u2764\ufe0f using NVIDIA Nemotron & RADIO foundation models*"
   ]
  }
 ]
}